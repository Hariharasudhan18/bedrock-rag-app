{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebc1c12",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 fitz cohere faiss-cpu anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21120692",
   "metadata": {},
   "source": [
    "## 2. Setup AWS S3 and Bedrock Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed894730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import faiss\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import cohere\n",
    "from anthropic import Client as AnthropicClient\n",
    "\n",
    "# Hardcoded AWS Credentials\n",
    "aws_access_key = 'YOUR_AWS_ACCESS_KEY'\n",
    "aws_secret_key = 'YOUR_AWS_SECRET_KEY'\n",
    "region_name = 'us-east-1'\n",
    "\n",
    "# Initialize S3 Client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Initialize Bedrock Client\n",
    "bedrock = boto3.client(\n",
    "    'bedrock',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Initialize Cohere Client\n",
    "cohere_client = cohere.Client(api_key=\"YOUR_COHERE_API_KEY\")\n",
    "\n",
    "# Initialize Anthropic Client\n",
    "anthropic_client = AnthropicClient(api_key=\"YOUR_ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44058ba2",
   "metadata": {},
   "source": [
    "## 3. Extract Text, Images, and Hyperlinks from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_links_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    links = []\n",
    "\n",
    "    for page_num in range(document.page_count):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "        links += page.get_links()\n",
    "    \n",
    "    return text, links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8b9d9",
   "metadata": {},
   "source": [
    "## 4. Recursive Character Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_split(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe8061",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings Using Cohere Embed English v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e943df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_chunks):\n",
    "    embeddings = cohere_client.embed(texts=text_chunks, model=\"embed-english-v3.0\")\n",
    "    return np.array(embeddings.embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b2bac",
   "metadata": {},
   "source": [
    "## 6. FAISS Vector Store and S3 Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings, dimension=4096):\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def save_faiss_index_to_s3(index, bucket_name, s3_key):\n",
    "    faiss.write_index(index, \"faiss_index.index\")\n",
    "    s3_client.upload_file(\"faiss_index.index\", bucket_name, s3_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3695b",
   "metadata": {},
   "source": [
    "## 7. Retrieve Relevant Chunks Using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query_embedding, index, text_chunks, k=5):\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [text_chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879707b2",
   "metadata": {},
   "source": [
    "## 8. ReAct Agent Logic with Anthropic Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, relevant_chunks, chat_history):\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"Given the following context: {context}\\nAnswer the following question: {query}\"\n",
    "    \n",
    "    # Include chat history\n",
    "    full_prompt = \"\\n\".join(chat_history + [prompt])\n",
    "\n",
    "    response = anthropic_client.completion(\n",
    "        model=\"sonnet-v1\",  # Use the appropriate Sonnet model\n",
    "        prompt=full_prompt,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    # Add the response to the chat history\n",
    "    chat_history.append(f\"User: {query}\")\n",
    "    chat_history.append(f\"Sonnet: {response['completion']}\")\n",
    "    \n",
    "    return response[\"completion\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116458d",
   "metadata": {},
   "source": [
    "## 9. Memory Buffer for Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da77d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def add_to_chat_history(query, response):\n",
    "    chat_history.append({\"query\": query, \"response\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95987e42",
   "metadata": {},
   "source": [
    "## 10. Guardrails for LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa41e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_guardrails(query):\n",
    "    allowed_keywords = [\"GenAI Hub\"]\n",
    "    if any(keyword.lower() in query.lower() for keyword in allowed_keywords):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def guarded_generate_response(query, relevant_chunks, chat_history):\n",
    "    if not apply_guardrails(query):\n",
    "        return \"Sorry, I can only answer questions related to GenAI Hub.\"\n",
    "\n",
    "    return generate_response(query, relevant_chunks, chat_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e409ae0",
   "metadata": {},
   "source": [
    "## 11. Evaluate Accuracy, Grounding, and Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4be147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_grounding(response, relevant_chunks, embedding_model):\n",
    "    response_embedding = embedding_model.embed(texts=[response])\n",
    "    chunk_embeddings = embedding_model.embed(texts=relevant_chunks)\n",
    "    \n",
    "    similarities = cosine_similarity(response_embedding, chunk_embeddings)\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def check_hallucination(response, relevant_chunks):\n",
    "    for chunk in relevant_chunks:\n",
    "        if response in chunk:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def final_check(response, relevant_chunks, embedding_model):\n",
    "    grounding_score = evaluate_grounding(response, relevant_chunks, embedding_model)\n",
    "    hallucination_detected = check_hallucination(response, relevant_chunks)\n",
    "    \n",
    "    if hallucination_detected or grounding_score < 0.5:\n",
    "        return \"Hmm, I don't know\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80da7d",
   "metadata": {},
   "source": [
    "## 12. Main Execution Logic in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pdf_path, query, bucket_name, s3_key):\n",
    "    # Step 1: Extract text and hyperlinks from the PDF\n",
    "    text, links = extract_text_and_links_from_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Split text using recursive splitter\n",
    "    text_chunks = recursive_split(text)\n",
    "\n",
    "    # Step 3: Generate embeddings using Cohere\n",
    "    embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "    # Step 4: Create FAISS index and upload to S3\n",
    "    index = create_faiss_index(embeddings)\n",
    "    save_faiss_index_to_s3(index, bucket_name, s3_key)\n",
    "\n",
    "    # Step 5: Retrieve relevant chunks using FAISS\n",
    "    query_embedding = generate_embeddings([query])[0].reshape(1, -1)\n",
    "    relevant_chunks = retrieve_relevant_chunks(query_embedding, index, text_chunks)\n",
    "\n",
    "    # Step 6: Generate response using ReAct agent and Anthropic\n",
    "    response = guarded_generate_response(query, relevant_chunks, chat_history)\n",
    "\n",
    "    # Step 7: Evaluate and detect hallucination\n",
    "    final_response = final_check(response, relevant_chunks, cohere_client)\n",
    "    \n",
    "    # Display response\n",
    "    print(final_response)\n",
    "\n",
    "# Execute\n",
    "pdf_path = \"path_to_your_pdf_file.pdf\"\n",
    "query = \"Your question here\"\n",
    "bucket_name = \"your-s3-bucket\"\n",
    "s3_key = \"faiss_index.index\"\n",
    "\n",
    "main(pdf_path, query, bucket_name, s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bc5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
